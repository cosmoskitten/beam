{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom Outputs - ParDo - Apache Beam",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wAFLwhVkcs_S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Custom outputs - ParDo\n",
        "\n",
        "`ParDo` is the recommended way to implement a Sink, since implementing one can be tricky."
      ]
    },
    {
      "metadata": {
        "id": "DOf-0hdvbH2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "First, let's install `apache-beam`."
      ]
    },
    {
      "metadata": {
        "id": "IvkH2xq_Fi-U",
        "colab_type": "code",
        "outputId": "f61b7325-18d9-48a2-8eb2-f9b91947b94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Run and print a shell command.\n",
        "def run(cmd):\n",
        "  print('>> {}'.format(cmd))\n",
        "  !{cmd}\n",
        "  print('')\n",
        "\n",
        "# Install apache-beam.\n",
        "run('pip install --quiet apache-beam')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> pip install --quiet apache-beam\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrlHKJAe0DHP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Example: Write to files\n",
        "\n",
        "A PCollection might contain more elements than what fit into memory in a single machine. So it's a good idea to break it into batches and then we can deal with each batch independently. To keep things simple, we'll create one file per batch.\n",
        "\n",
        "A very simple batching strategy is to just assign each element to a random batch."
      ]
    },
    {
      "metadata": {
        "id": "j53b2lUOPdN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "046b4cdc-015a-4e39-e08c-d0353e446682"
      },
      "cell_type": "code",
      "source": [
        "run('rm -rf outputs')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> rm -rf outputs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WJd4YPMPM4ty",
        "colab_type": "code",
        "outputId": "ede672b0-da4c-4ef9-b4ab-dc5a98988976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import logging\n",
        "import random\n",
        "import os\n",
        "\n",
        "if not os.path.exists('outputs'):\n",
        "  os.makedirs('outputs')\n",
        "\n",
        "def write_to_file(key_value):\n",
        "  batch_idx, values = key_value\n",
        "  with open('outputs/part-{}'.format(batch_idx), 'w') as f:\n",
        "    for value in values:\n",
        "      f.write('{}\\n'.format(value))\n",
        "\n",
        "# Running locally in the DirectRunner.\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (\n",
        "      pipeline\n",
        "      | 'Create inputs' >> beam.Create([i for i in range(10)])\n",
        "      | 'Set key to a randomized batch number' >> beam.Map(\n",
        "          lambda element: (random.randint(1, 3), element))\n",
        "      | 'Group into batches' >> beam.GroupByKey()\n",
        "      | 'Write to files' >> beam.Map(write_to_file)\n",
        "  )\n",
        "\n",
        "# Check the outputs.\n",
        "!ls -lh outputs/\n",
        "!head outputs/part*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 12K\n",
            "-rw-r--r-- 1 root root  6 Jan 30 00:41 part-1\n",
            "-rw-r--r-- 1 root root 12 Jan 30 00:41 part-2\n",
            "-rw-r--r-- 1 root root  2 Jan 30 00:41 part-3\n",
            "==> outputs/part-1 <==\n",
            "0\n",
            "1\n",
            "5\n",
            "\n",
            "==> outputs/part-2 <==\n",
            "2\n",
            "3\n",
            "4\n",
            "7\n",
            "8\n",
            "9\n",
            "\n",
            "==> outputs/part-3 <==\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CMC7wBYsMFrY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, this requires us to know the number of batches we want to split our data into which might not be possible in a streaming scenario.\n",
        "\n",
        "Another option is to create batches as elements arrive and yield a list of elements once it has reached the desired size."
      ]
    },
    {
      "metadata": {
        "id": "0IYUbGkmPfek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "827bb84f-82c0-40f9-f166-80ebc1abb166"
      },
      "cell_type": "code",
      "source": [
        "run('rm -rf outputs')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> rm -rf outputs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cELzsVNfLflu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a1c5cae3-7f2a-4ee4-f9d7-5f370a8fe70d"
      },
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import logging\n",
        "import os\n",
        "\n",
        "if not os.path.exists('outputs'):\n",
        "  os.makedirs('outputs')\n",
        "\n",
        "def write_to_file(key_value):\n",
        "  batch_idx, values = key_value\n",
        "  with open('outputs/part-{}'.format(batch_idx), 'w') as f:\n",
        "    for value in values:\n",
        "      f.write('{}\\n'.format(value))\n",
        "\n",
        "class GroupIntoBatches(beam.DoFn):\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "    self.buffer = []\n",
        "    self.batch_idx = 0\n",
        "\n",
        "  def process(self, element):\n",
        "    self.buffer.append(element)\n",
        "    if len(self.buffer) == self.n:\n",
        "      yield self.batch_idx, list(self.buffer)\n",
        "      self.buffer = []\n",
        "      self.batch_idx += 1\n",
        "\n",
        "  def finish_bundle(self):\n",
        "    if len(self.buffer) != 0:\n",
        "      value = self.batch_idx, list(self.buffer)\n",
        "      yield beam.utils.windowed_value.WindowedValue(value, -1, [])\n",
        "\n",
        "# Running locally in the DirectRunner.\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (\n",
        "      pipeline\n",
        "      | 'Create inputs' >> beam.Create([i for i in range(10)])\n",
        "      | 'Group into batches' >> beam.ParDo(GroupIntoBatches(3))\n",
        "      | 'Write to files' >> beam.Map(write_to_file)\n",
        "  )\n",
        "\n",
        "# Check the outputs.\n",
        "!ls -lh outputs/\n",
        "!head outputs/part*"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16K\n",
            "-rw-r--r-- 1 root root 6 Jan 30 00:52 part-0\n",
            "-rw-r--r-- 1 root root 6 Jan 30 00:52 part-1\n",
            "-rw-r--r-- 1 root root 6 Jan 30 00:52 part-2\n",
            "-rw-r--r-- 1 root root 2 Jan 30 00:52 part-3\n",
            "==> outputs/part-0 <==\n",
            "0\n",
            "1\n",
            "2\n",
            "\n",
            "==> outputs/part-1 <==\n",
            "3\n",
            "4\n",
            "5\n",
            "\n",
            "==> outputs/part-2 <==\n",
            "6\n",
            "7\n",
            "8\n",
            "\n",
            "==> outputs/part-3 <==\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}